{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cdmpPuMG5E8KtkflXZ18zH8cU92cyN5G",
      "authorship_tag": "ABX9TyNG/A3GNqHH+kUL5oQcJhRh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akira-harada/SPECIAL_BLEND/blob/main/SPECIAL_BLEND_pyinterface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{user_name}:{access_token}@github.com/akira-harada/SPECIAL_BLEND.git"
      ],
      "metadata": {
        "id": "VIOos1JZF0zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "0V7EP95lWyc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!{sys.executable} -m numpy.f2py --quiet -c /content/SPECIAL_BLEND/SPECIAL_BLEND.f90 -m SPECIAL_BLEND"
      ],
      "metadata": {
        "id": "kO7YIeO_UQX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Skip this cell when you use your own observational data.\n",
        "%run /content/SPECIAL_BLEND/event_generator.py"
      ],
      "metadata": {
        "id": "XEolo5nL0Eqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# define functions\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import numpy as np\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import SPECIAL_BLEND as SB\n",
        "\n",
        "def main():\n",
        "  params = np.loadtxt('/content/SPECIAL_BLEND/parameters.dat') \n",
        "  # 'parameters.dat' file has the following contents:\n",
        "  # assumed gbeta,\n",
        "  # distance to the SN [kpc],\n",
        "  # detector mass [kton],\n",
        "  # parameter search grid number,\n",
        "  # minimum search mass [Msun],\n",
        "  # maximum search mass [Msun],\n",
        "  # minimum search radius [km],\n",
        "  # maximum search radius [km],\n",
        "  # minimum search energy [erg],\n",
        "  # and maximum search energy [erg]\n",
        "  origdata = np.loadtxt('/content/time_energy.dat') \n",
        "  # 'time_energy.dat' file has the time and energy of each event: first column is time, second column is energy\n",
        "  analysis_mode = int(params[10])# 1:unbinned, 2:binned, 3:Gaussian approximation\n",
        "  tmin = params[13]\n",
        "  tmax = params[14]\n",
        "  data = loaddata(tmin,tmax,origdata)\n",
        "  if analysis_mode == 1:\n",
        "    print(\"unbinned analysis\")\n",
        "    mlogLH,mass,rad,et = unbinned_likelihood(data,params)\n",
        "    print(\"likelihood calculation completed\")\n",
        "  elif analysis_mode == 2:\n",
        "    print(\"binned analysis\")\n",
        "    mlogLH,mass,rad,et = binned_likelihood(data,params)\n",
        "    print(\"likelihood calculation completed\")\n",
        "  elif analysis_mode == 3:\n",
        "    print(\"Gaussian-approximation analysis\")\n",
        "    mlogLH,mass,rad,et = GA_likelihood(data,params)\n",
        "    print(\"likelihood calculation completed\")\n",
        "  else:\n",
        "    print(\"invalid analysis mode\")\n",
        "  return mass,rad,et,mlogLH\n",
        "  \n",
        "\n",
        "def loaddata(tmin,tmax,origdata):\n",
        "  cuttdata = origdata[:,0].clip(tmin,tmax)\n",
        "  diftdata = cuttdata - origdata[:,0]\n",
        "  islice = np.where(diftdata==0.0)[0].min()\n",
        "  fslice = np.where(diftdata==0.0)[0].max()+1\n",
        "  data = origdata[islice:fslice,:]\n",
        "  origsize = origdata[:,0].shape[0]\n",
        "  datsize = data.shape[0]\n",
        "  print(\"used event number {:d} /total event number {:d}\".format(datsize,origsize))\n",
        "  print(\"{:d} events are outside [tmin,tmax] and neglected\".format(origsize-datsize))\n",
        "  return data\n",
        "\n",
        "\n",
        "def unbinned_likelihood(data,params):\n",
        "  datsize = data.shape[0]\n",
        "  tdata = data[:,0]\n",
        "  edata = data[:,1]\n",
        "  gbeta = params[0]\n",
        "  dist = params[1]\n",
        "  Mdet = params[2]\n",
        "  nparam = int(params[3])\n",
        "  mmin = params[4]\n",
        "  mmax = params[5]\n",
        "  rmin = params[6]\n",
        "  rmax = params[7]\n",
        "  emin = params[8]\n",
        "  emax = params[9]\n",
        "\n",
        "  mass = np.linspace(mmin,mmax,nparam)\n",
        "  rad  = np.linspace(rmin,rmax,nparam)\n",
        "  et   = np.linspace(emin,emax,nparam)\n",
        "\n",
        "  mlogLH = SB.eval_unbinned_likelihood(tdata,edata,mass,rad,et,gbeta,dist,Mdet)\n",
        "    \n",
        "  return mlogLH,mass,rad,et\n",
        "\n",
        "\n",
        "def binned_likelihood(data,params):\n",
        "  datsize = data.shape[0]\n",
        "  tdata = data[:,0]\n",
        "  edata = data[:,1]\n",
        "  gbeta = params[0]\n",
        "  dist = params[1]\n",
        "  Mdet = params[2]\n",
        "  nparam = int(params[3])\n",
        "  mmin = params[4]\n",
        "  mmax = params[5]\n",
        "  rmin = params[6]\n",
        "  rmax = params[7]\n",
        "  emin = params[8]\n",
        "  emax = params[9]\n",
        "  tbinnumber = int(params[11])\n",
        "  dt_ini = params[12]\n",
        "  tmin   = params[13]\n",
        "  tmax   = params[14]\n",
        "  \n",
        "  t_bin,dt,e_bin,de,hist,status = SB.binning(tdata,edata,dt_ini,tmin,tmax,tbinnumber)\n",
        "  if status >= 1:\n",
        "    print(\"binning error, try another bin number\")\n",
        "    interrupt_by_undefined_function()\n",
        "  \n",
        "  mass = np.linspace(mmin,mmax,nparam)\n",
        "  rad  = np.linspace(rmin,rmax,nparam)\n",
        "  et   = np.linspace(emin,emax,nparam)\n",
        "\n",
        "  mlogLH = SB.eval_binned_likelihood(t_bin,dt,e_bin,de,hist,mass,rad,et,gbeta,dist,Mdet)\n",
        "  return mlogLH,mass,rad,et\n",
        "\n",
        "def GA_likelihood(data,params):\n",
        "  datsize = data.shape[0]\n",
        "  tdata = data[:,0]\n",
        "  edata = data[:,1]\n",
        "  gbeta = params[0]\n",
        "  dist = params[1]\n",
        "  Mdet = params[2]\n",
        "  nparam = int(params[3])\n",
        "  mmin = params[4]\n",
        "  mmax = params[5]\n",
        "  rmin = params[6]\n",
        "  rmax = params[7]\n",
        "  emin = params[8]\n",
        "  emax = params[9]\n",
        "  tbinnumber = int(params[11])\n",
        "  dt_ini = params[12]\n",
        "  tmin   = params[13]\n",
        "  tmax   = params[14]\n",
        "  \n",
        "  t_bin,dt,thist,status = SB.time_binning(tdata,dt_ini,tmin,tmax,tbinnumber)\n",
        "  if status >= 1:\n",
        "    print(\"time binning error, try another bin number\")\n",
        "    interrupt_by_undefined_function()\n",
        "  e_ave,status = SB.event_energy_averaging(thist,edata)\n",
        "  if status >= 1:\n",
        "    print(\"{:d}-th time bin has no events\".format(status))\n",
        "    print(\"try another time bin number\")\n",
        "    interrupt_by_undefined_function()\n",
        "\n",
        "  mass = np.linspace(mmin,mmax,nparam)\n",
        "  rad  = np.linspace(rmin,rmax,nparam)\n",
        "  et   = np.linspace(emin,emax,nparam)\n",
        "\n",
        "  mlogLH = SB.eval_gaussian_likelihood(t_bin,dt,thist,e_ave,mass,rad,et,gbeta,dist,Mdet)\n",
        "  return mlogLH,mass,rad,et\n",
        "\n",
        "def marginalizeLH(mass,rad,et,mlogLH):\n",
        "  LH_MR,CI95level,CI68level,peak = SB.mr_marginalize(mass,rad,et,mlogLH)\n",
        "  MRvals  = [CI95level,CI68level,peak[0],peak[1]]\n",
        "  LH_RE,CI95level,CI68level,peak = SB.re_marginalize(mass,rad,et,mlogLH)\n",
        "  REvals  = [CI95level,CI68level,peak[0],peak[1]]\n",
        "  LH_EM,CI95level,CI68level,peak = SB.em_marginalize(mass,rad,et,mlogLH)\n",
        "  EMvals  = [CI95level,CI68level,peak[0],peak[1]]\n",
        "  LH_M,CIBFM  = SB.m_marginalize(mass,rad,et,mlogLH)\n",
        "  LH_R,CIBFR  = SB.r_marginalize(mass,rad,et,mlogLH)\n",
        "  LH_E,CIBFE  = SB.e_marginalize(mass,rad,et,mlogLH)\n",
        "  print(\"1D marginalized result\")\n",
        "  print(\"mass = {:e} +{:.2e}/-{:.2e} (68%) +{:.2e}/-{:.2e} (95%)\".format(CIBFM[2],CIBFM[3]-CIBFM[2],CIBFM[2]-CIBFM[1],CIBFM[4]-CIBFM[2],CIBFM[2]-CIBFM[0]))\n",
        "  if (CIBFM[0] > CIBFM[1]) or (CIBFM[1] > CIBFM[2]) or (CIBFM[2] > CIBFM[3]) or (CIBFM[3] > CIBFM[4]):\n",
        "    print('\\033[35m'+\"1D CI estimation error in function marginalizeLH()\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"array CIBFM containes [lower edge of 95%-CI, lower edge of 68%-CI, best fit, upper edge of 68%-CI, upper edge of 95%-CI]\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"but ascending nature of CIBFM does not hold\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"retry with another parameter search range\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"or data quality is too low to analyze\"+'\\033[0m')\n",
        "  print(\"radius = {:e} +{:.2e}/-{:.2e} (68%) +{:.2e}/-{:.2e} (95%)\".format(CIBFR[2],CIBFR[3]-CIBFR[2],CIBFR[2]-CIBFR[1],CIBFR[4]-CIBFR[2],CIBFR[2]-CIBFR[0]))\n",
        "  if (CIBFR[0] > CIBFR[1]) or (CIBFR[1] > CIBFR[2]) or (CIBFR[2] > CIBFR[3]) or (CIBFR[3] > CIBFR[4]):\n",
        "    print('\\033[35m'+\"1D CI estimation error in function marginalizeLH()\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"array CIBFR containes [lower edge of 95%-CI, lower edge of 68%-CI, best fit, upper edge of 68%-CI, upper edge of 95%-CI]\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"but ascending nature of CIBFR does not hold\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"retry with another parameter search range\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"or data quality is too low to analyze\"+'\\033[0m')\n",
        "  print(\"energy = {:e} +{:.2e}/-{:.2e} (68%) +{:.2e}/-{:.2e} (95%)\".format(CIBFE[2],CIBFE[3]-CIBFE[2],CIBFE[2]-CIBFE[1],CIBFE[4]-CIBFE[2],CIBFE[2]-CIBFE[0]))\n",
        "  if (CIBFE[0] > CIBFE[1]) or (CIBFE[1] > CIBFE[2]) or (CIBFE[2] > CIBFE[3]) or (CIBFE[3] > CIBFE[4]):\n",
        "    print('\\033[35m'+\"1D CI estimation error in function marginalizeLH()\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"array CIBFE containes [lower edge of 95%-CI, lower edge of 68%-CI, best fit, upper edge of 68%-CI, upper edge of 95%-CI]\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"but ascending nature of CIBFR does not hold\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"retry with another parameter search range\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"or data quality is too low to analyze\"+'\\033[0m')\n",
        "  print()\n",
        "  print(\"2D marginalized result\")\n",
        "  print(\"M-R: the best fit is (M,R)=({:.2e},{:.2e}), and the levels of CI is {:.2e} (68%) and {:.2e} (95%)\".format(MRvals[2],MRvals[3],MRvals[1],MRvals[0]))\n",
        "  if (LH_MR.max() < MRvals[0]) or (LH_MR.max() < MRvals[1]) or (MRvals[0] > MRvals[1]):\n",
        "    print('\\033[35m'+\"2D CI estimation error in function marginalizeLH()\"+'\\033[0m')\n",
        "    if (LH_MR.max() < MRvals[0]) or (LH_MR.max() < MRvals[1]):\n",
        "      print('\\033[35m'+\"levels of CI exceeds maximum value of the likelihood\"+'\\033[0m')\n",
        "    if (MRvals[0] > MRvals[1]):\n",
        "      print('\\033[35m'+\"95%-CI contour may be inside 68%-CI contour\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"retry with another parameter search range\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"or data quality is too low to analyze\"+'\\033[0m')\n",
        "  print(\"R-E: the best fit is (R,E)=({:.2e},{:.2e}), and the levels of CI is {:.2e} (68%) and {:.2e} (95%)\".format(REvals[2],REvals[3],REvals[1],REvals[0]))\n",
        "  if (LH_RE.max() < REvals[0]) or (LH_RE.max() < REvals[1]) or (REvals[0] > REvals[1]):\n",
        "    print('\\033[35m'+\"2D CI estimation error in function marginalizeLH()\"+'\\033[0m')\n",
        "    if (LH_RE.max() < REvals[0]) or (LH_RE.max() < REvals[1]):\n",
        "      print('\\033[35m'+\"levels of CI exceeds maximum value of the likelihood\"+'\\033[0m')\n",
        "    if (REvals[0] > REvals[1]):\n",
        "      print('\\033[35m'+\"95%-CI contour may be inside 68%-CI contour\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"retry with another parameter search range\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"or data quality is too low to analyze\"+'\\033[0m')\n",
        "  print(\"E-M: the best fit is (E,M)=({:.2e},{:.2e}), and the levels of CI is {:.2e} (68%) and {:.2e} (95%)\".format(EMvals[2],EMvals[3],EMvals[1],EMvals[0]))\n",
        "  if (LH_EM.max() < EMvals[0]) or (LH_EM.max() < EMvals[1]) or (EMvals[0] > EMvals[1]):\n",
        "    print('\\033[35m'+\"2D CI estimation error in function marginalizeLH()\"+'\\033[0m')\n",
        "    if (LH_EM.max() < EMvals[0]) or (LH_EM.max() < EMvals[1]):\n",
        "      print('\\033[35m'+\"levels of CI exceeds maximum value of the likelihood\"+'\\033[0m')\n",
        "    if (EMvals[0] > EMvals[1]):\n",
        "      print('\\033[35m'+\"95%-CI contour may be inside 68%-CI contour\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"retry with another parameter search range\"+'\\033[0m')\n",
        "    print('\\033[35m'+\"or data quality is too low to analyze\"+'\\033[0m')\n",
        "  save_likelihood(mass,rad,et,mlogLH,LH_MR,LH_RE,LH_EM,LH_M,LH_R,LH_E)\n",
        "\n",
        "  return LH_MR,MRvals,LH_RE,REvals,LH_EM,EMvals,LH_M,LH_R,LH_E\n",
        "\n",
        "\n",
        "def save_likelihood(mass,rad,et,mlogLH,LH_MR,LH_RE,LH_EM,LH_M,LH_R,LH_E):\n",
        "  Marray = np.stack([mass,LH_M],1)\n",
        "  Mhead = [['mass',  'LH_M']]\n",
        "  with open(\"LH_M.csv\",\"w\",newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(np.concatenate([Mhead,Marray],0))\n",
        "  \n",
        "  Rarray = np.stack([rad, LH_R],1)\n",
        "  Rhead = [['radius','LH_R']]\n",
        "  with open(\"LH_R.csv\",\"w\",newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(np.concatenate([Mhead,Marray],0))\n",
        "  \n",
        "  Earray = np.stack([et,  LH_E],1)\n",
        "  Ehead = [['energy','LH_E']]\n",
        "  with open(\"LH_E.csv\",\"w\",newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(np.concatenate([Mhead,Marray],0))\n",
        "\n",
        "  mrm,mrr = np.meshgrid(mass,rad)\n",
        "  rer,ree = np.meshgrid(rad,et)\n",
        "  eme,emm = np.meshgrid(et,mass)\n",
        "\n",
        "  mrm1    = np.ravel(mrm)\n",
        "  mrr1    = np.ravel(mrr)\n",
        "  LH_MR1  = np.ravel(LH_MR)\n",
        "  MRarray = np.stack([mrm1,mrr1,LH_MR1]).T\n",
        "  MRhead  = [['mass','radius','LH_MR']]\n",
        "  with open(\"LH_MR.csv\",\"w\",newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(np.concatenate([MRhead,MRarray],0))\n",
        "\n",
        "  rer1    = np.ravel(rer)\n",
        "  ree1    = np.ravel(ree)\n",
        "  LH_RE1  = np.ravel(LH_RE)\n",
        "  REarray = np.stack([rer1,ree1,LH_RE1]).T\n",
        "  REhead  = [['radius','energy','LH_RE']]\n",
        "  with open(\"LH_RE.csv\",\"w\",newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(np.concatenate([REhead,REarray],0))\n",
        "\n",
        "  eme1    = np.ravel(eme)\n",
        "  emm1    = np.ravel(emm)\n",
        "  LH_EM1  = np.ravel(LH_EM)\n",
        "  EMarray = np.stack([eme1,emm1,LH_EM1]).T\n",
        "  EMhead  = [['energy','mass','LH_EM']]\n",
        "  with open(\"LH_EM.csv\",\"w\",newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(np.concatenate([EMhead,EMarray],0))\n",
        "\n",
        "\n",
        "def visualize(mass,rad,et,mlogLH,LH_MR,MRvals,LH_RE,REvals,LH_EM,EMvals,LH_M,LH_R,LH_E):\n",
        "  mrm,mrr = np.meshgrid(mass,rad)\n",
        "  plt.contourf(mrm,mrr,LH_MR.T)\n",
        "  plt.contour(mrm,mrr,LH_MR.T,colors=['white'],levels=[MRvals[0],MRvals[1]])\n",
        "  plt.plot(MRvals[2],MRvals[3],color='white',marker='.',markersize=10)\n",
        "  plt.xlabel(r\"mass [$M_\\odot$]\")\n",
        "  plt.ylabel(r\"radius [km]\")\n",
        "  plt.show()\n",
        "\n",
        "  rer,ree = np.meshgrid(rad,et)\n",
        "  plt.contourf(rer,ree,LH_RE.T)\n",
        "  plt.contour(rer,ree,LH_RE.T,colors=['white'],levels=[REvals[0],REvals[1]])\n",
        "  plt.plot(REvals[2],REvals[3],color='white',marker='.',markersize=10)\n",
        "  plt.xlabel(r\"radius [km]\")\n",
        "  plt.ylabel(r\"energy [erg]\")\n",
        "  plt.show()\n",
        "\n",
        "  eme,emm = np.meshgrid(et,mass)\n",
        "  plt.contourf(eme,emm,LH_EM.T)\n",
        "  plt.contour(eme,emm,LH_EM.T,colors=['white'],levels=[EMvals[0],EMvals[1]])\n",
        "  plt.plot(EMvals[2],EMvals[3],color='white',marker='.',markersize=10)\n",
        "  plt.xlabel(r\"energy [erg]\")\n",
        "  plt.ylabel(r\"mass [$M_\\odot$]\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(mass,LH_M)\n",
        "  plt.xlabel(r\"mass [$M_\\odot$]\")\n",
        "  plt.show()\n",
        "  print()\n",
        "\n",
        "  plt.plot(rad,LH_R)\n",
        "  plt.xlabel(r\"radius [km]\")\n",
        "  plt.show()\n",
        "  print()\n",
        "\n",
        "  plt.plot(et,LH_E)\n",
        "  plt.xlabel(r\"energy [erg]\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "f1KGhx3Z63iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate likelihood (mlogLH = minus log likelihood)\n",
        "%time mass,rad,et,mlogLH=main()"
      ],
      "metadata": {
        "id": "NhGzKu0bmvp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# marginalize likelihood\n",
        "LH_MR,MRvals,LH_RE,REvals,LH_EM,EMvals,LH_M,LH_R,LH_E = marginalizeLH(mass,rad,et,mlogLH)"
      ],
      "metadata": {
        "id": "idMrnDbaodbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize likelihood\n",
        "visualize(mass,rad,et,mlogLH,LH_MR,MRvals,LH_RE,REvals,LH_EM,EMvals,LH_M,LH_R,LH_E)"
      ],
      "metadata": {
        "id": "LF2ub-Wy2a5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cqbIYnege2GX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}